---
title: "Text Mining"
output:
  html_document:
    df_print: paged
date: "03-03-2020"
---

# Text Mining

There are quite a number of R packages for doing text mining. For example,

- tm
- quanteda
- tidytext
- corpus
- koRpus

See here for a nice comparsion between the packages.

We will focus on `tidytext` for its simplicity.


So what do we do with text mining?

(from https://www.linguamatics.com/)
> Text mining (also referred to as text analytics) is an artificial intelligence (AI) technology that uses natural language processing (NLP) to transform the free (unstructured) text in documents and databases into normalized, structured data suitable for analysis or to drive machine learning (ML) algorithms.

(Disclaimer: I am not an expert of NPL or text mining)

# Data structures

Text data often are stored in the following ways

- String: Text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form.
- Corpus: These types of objects typically contain raw strings annotated with additional metadata and details.
- Document-term matrix: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count or tf-idf 


```{r}
library(tidyverse)
library(tidytext)
```

```{r}
# A poem by Emily Dickinson 
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")
df <- tibble(line = 1:4, text = text)
```

We want to convert the data frame so that it has one-token-per-document-per-row.

```{r}
df %>% 
  unnest_tokens(words, text)
```

After using unnest_tokens, we've split each row so that there is one token (word) in each row of the new data frame; the default tokenization in `unnest_tokens()` is for single words, as shown here. Also notice:

- Other columns, such as the line number each word came from, are retained.
- Punctuation has been stripped.
- By default, `unnest_tokens()` converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the `to_lower = FALSE` argument to turn off this behavior).



Often in text analysis, we will want to remove stop words; stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth in English.
```{r}
data(stop_words)
stop_words
```


```{r}
library(rvest)
```


Try to read some news headlines from yahoo
```{r, cache = TRUE}
h3 <- read_html("http://news.yahoo.com/") %>% 
  html_nodes("h3")
news_df <- tibble(
  title = h3 %>% html_text(),
  url = h3 %>%  html_node("a") %>% html_attr("href")
)
news_df <- news_df %>% 
  filter(str_starts(url, "/")) %>% 
  mutate(url = str_c("http://news.yahoo.com", url), id = row_number()) %>% 
  head(10)
```

Read the contents
```{r, cache = TRUE}
news_content <- NULL
for (i in seq_len(nrow(news_df))) {
  news_content <- bind_rows(news_content, read_html(news_df$url[i]) %>% 
    html_nodes("article p") %>% 
    html_text() %>% 
    tibble(id = news_df$id[i], text = .))
}
```

```{r}
news_tokens <- news_content %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  group_by(id) %>% 
  count(word, sort = TRUE)
```

```{r}
(news_propotions <- news_tokens %>% 
  group_by(id) %>% 
  mutate(propotion = n / sum(n)) %>% 
  select(-n) %>% 
  arrange(id, desc(propotion)))
```


```{r, message = FALSE}
library(scales)
```

```{r}
news_propotions %>% 
  pivot_wider(names_from = id, names_prefix = "prop", values_from = propotion, values_fill = list(propotion = 0)) %>% 
  pivot_longer(prop2:last_col(), names_to = "id", names_prefix = "prop", values_to = "prop") %>% 
  ggplot(aes(prop, prop1)) + 
    geom_point() +
    geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
    scale_x_sqrt() +
    scale_y_sqrt() +  
    xlab(NULL) +
    facet_wrap(~id)
```

```{r}
library(wordcloud)
news_tokens %>% 
  filter(id == 1) %>% 
  with(wordcloud(word, n, max.words = 100))
```



## Sentiment analysis

The `tidytext` package contains several sentiment lexicons. Three general-purpose lexicons are

- `AFINN` from Finn Årup Nielsen,
- `bing` from Bing Liu and collaborators, and
- `nrc` from Saif Mohammad and Peter Turney.

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. 
It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in “no good” or “not true”; a lexicon-based method like this is based on unigrams only.


```{r}
library(textdata)

get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```


```{r}
news_tokens %>% 
  left_join(get_sentiments("bing")) %>% 
  group_by(id) %>% 
  summarise(
    positive = sum(sentiment == "positive", na.rm = TRUE), 
    negative = sum(sentiment == "negative", na.rm = TRUE), 
    na = n() - positive - negative) %>%
  mutate(
    id,
    sentiment = case_when(
      positive > negative ~ "positive",
      positive < negative ~ "negative",
      TRUE ~ "unknown"
    )
  ) %>% 
  left_join(select(news_df, id, title))
```

# Analyzing word and document frequency: tf-idf

A central question in text mining and natural language processing is how to quantify what a document is about.
One measure of how important a word may be is its term frequency. Another approach is to look at a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much.

$$
\text{idf} = \ln\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}} \right)
$$

$$
\text{tf_idf} = \text{tf} \times \text{idf}
$$

Show the 3 most important words in each document.
```{r}
news_tokens %>% 
  bind_tf_idf(word, id, n) %>%
  arrange(id) %>% 
  filter(n() - row_number(tf_idf) + 1 <= 3)
```

Using term frequency and inverse document frequency allows us to find words that are characteristic for one document within a collection of documents.

## n-grams

We’ve been using the unnest_tokens function to tokenize by word, or sometimes by sentence, which is useful for the kinds of sentiment and frequency analyses we’ve been doing so far. But we can also use the function to tokenize into consecutive sequences of words, called n-grams.


```{r}
derivative_bigrams <- read_html("https://en.wikipedia.org/wiki/Derivative") %>% 
  html_nodes("p") %>% 
  html_text() %>% 
  tibble(text = .) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  anti_join(stop_words, by = c("word1" = "word")) %>% 
  anti_join(stop_words, by = c("word2" = "word")) %>% 
  unite(bigram, sep = " ")
```

```{r}
derivative_bigrams %>%
  count(bigram, sort = TRUE)
```

```{r}
news_content %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  anti_join(stop_words, by = c("word1" = "word")) %>% 
  anti_join(stop_words, by = c("word2" = "word")) %>% 
  group_by(id) %>% 
  count(word1, word2, sort = TRUE) %>% 
  arrange(id)
```


# Using bigrams to provide context in sentiment analysis

Our sentiment analysis approach above simply counted the appearance of positive or negative words. One of the problems with this approach is that a word’s context can matter nearly as much as its presence. For example, the words “happy” and “like” will be counted as positive, even in a sentence like “I’m not happy and I don’t like it!”


```{r}
stop_words_without_not <- stop_words %>% filter(word != "not")
```


```{r}
# stop words are not removed in this case
news_tokens2 <- news_content %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
```

Use 2-grams to do sentiment analysis
```{r}
news_tokens2 %>% 
  left_join(get_sentiments("bing"), by = c("word2" = "word")) %>%
  mutate(if_else(word1 == "not" && sentiment == "negative", "positive", "negative")) %>% 
  mutate(if_else(word1 == "not" && sentiment == "positive", "negative", "positive")) %>% 
  group_by(id) %>% 
  summarise(
    positive = sum(sentiment == "positive", na.rm = TRUE), 
    negative = sum(sentiment == "negative", na.rm = TRUE), 
    na = n() - positive - negative)
```

## LDA

Latent Dirichlet allocation is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.

- Every topic is a mixture of words
- Every document is a mixture of topics

```{r}
library(topicmodels)
```

```{r}
news_lda <- LDA(
  cast_dtm(news_tokens, id, word, n), 
  k = 2, 
  control = list(seed = 1234) )
```

### Word-topic probabilities
```{r}
tidy(news_lda, matrix = "beta")
```

### Document-topic probabilities


```{r}
tidy(news_lda, matrix = "gamma")
```

# Reference:

- Tidytext: https://www.tidytextmining.com/